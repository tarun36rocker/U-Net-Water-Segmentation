{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Downloads"
      ],
      "metadata": {
        "id": "VpIt3YUZlS1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neob-3BSS6sc"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGe4H-f9TlXX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT-syXNUVLL6"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS983aUzWpAo"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giz2BcEMWrfN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sQisoMXWt8G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Step 4: Download dataset\n",
        "!kaggle datasets download -d franciscoescobar/satellite-images-of-water-bodies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0a-u7U9W2wI"
      },
      "outputs": [],
      "source": [
        "# Step 5: Unzip the dataset\n",
        "!unzip satellite-images-of-water-bodies.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing"
      ],
      "metadata": {
        "id": "ptbK_ePCljmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjpqK5JmGzfN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall, MeanIoU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import cv2\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths to your dataset\n",
        "IMAGE_PATH = '/content/Water Bodies Dataset/Images'\n",
        "MASK_PATH = '/content/Water Bodies Dataset/Masks'\n",
        "IMAGE_SIZE = (256, 256)  # Resize all images to 256x256\n",
        "\n",
        "# Helper function to load image and mask names from the text files\n",
        "def load_file_list(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        file_list = f.read().splitlines()\n",
        "    return file_list\n",
        "\n",
        "# Load image names from the train.txt, val.txt, and test.txt files\n",
        "train_image_list = load_file_list('train.txt')\n",
        "val_image_list = load_file_list('val.txt')\n",
        "test_image_list = load_file_list('test.txt')\n",
        "\n",
        "# Append the full path to the image and mask names\n",
        "def append_paths(image_list, image_path, mask_path):\n",
        "    images = [os.path.join(image_path, img) for img in image_list]\n",
        "    masks = [os.path.join(mask_path, img) for img in image_list]\n",
        "    return images, masks\n",
        "\n",
        "# Create the full paths for train, val, and test images and masks\n",
        "image_train, mask_train = append_paths(train_image_list, IMAGE_PATH, MASK_PATH)\n",
        "image_val, mask_val = append_paths(val_image_list, IMAGE_PATH, MASK_PATH)\n",
        "image_test, mask_test = append_paths(test_image_list, IMAGE_PATH, MASK_PATH)\n",
        "\n",
        "# Print the number of train, validation, and test images and masks\n",
        "num_train = len(image_train)\n",
        "num_val = len(image_val)\n",
        "num_test = len(image_test)\n",
        "\n",
        "print(f\"Number of training images: {num_train}\")\n",
        "print(f\"Number of validation images: {num_val}\")\n",
        "print(f\"Number of test images: {num_test}\")\n",
        "\n",
        "# Calculate and print the percentage splits\n",
        "total_images = num_train + num_val + num_test\n",
        "train_split = (num_train / total_images) * 100\n",
        "val_split = (num_val / total_images) * 100\n",
        "test_split = (num_test / total_images) * 100\n",
        "\n",
        "print(f\"Train split: {train_split:.2f}%\")\n",
        "print(f\"Validation split: {val_split:.2f}%\")\n",
        "print(f\"Test split: {test_split:.2f}%\")\n",
        "\n",
        "# Function to resize and load images in batches\n",
        "def data_generator(image_files, mask_files, batch_size, image_size):\n",
        "    while True:\n",
        "        for i in range(0, len(image_files), batch_size):\n",
        "            batch_image_files = image_files[i:i + batch_size]\n",
        "            batch_mask_files = mask_files[i:i + batch_size]\n",
        "\n",
        "            images = []\n",
        "            masks = []\n",
        "\n",
        "            for img_file, mask_file in zip(batch_image_files, batch_mask_files):\n",
        "                # Load the image and mask\n",
        "                img = cv2.imread(img_file)\n",
        "                mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                # Resize both image and mask\n",
        "                img = cv2.resize(img, image_size)\n",
        "                mask = cv2.resize(mask, image_size)\n",
        "\n",
        "                # Normalize image and binary mask\n",
        "                img = img / 255.0\n",
        "                mask = mask / 255.0\n",
        "\n",
        "                images.append(img)\n",
        "                masks.append(np.expand_dims(mask, axis=-1))  # Add an extra dimension for the mask\n",
        "\n",
        "            yield np.array(images), np.array(masks)\n",
        "\n",
        "# Parameters for data generator\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Create data generators\n",
        "train_gen = data_generator(image_train, mask_train, BATCH_SIZE, IMAGE_SIZE)\n",
        "val_gen = data_generator(image_val, mask_val, BATCH_SIZE, IMAGE_SIZE)\n",
        "test_gen = data_generator(image_test, mask_test, BATCH_SIZE, IMAGE_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition and Training"
      ],
      "metadata": {
        "id": "ajuLAZmml4B0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8gs7bWTGzaQ"
      },
      "outputs": [],
      "source": [
        "# Define the U-Net architecture\n",
        "def unet_model(input_size):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Encoding path (Contracting)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    # Decoding path (Expansive)\n",
        "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
        "    up6 = Conv2D(512, 2, activation='relu', padding='same')(up6)\n",
        "    merge6 = concatenate([drop4, up6], axis=3)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    up7 = Conv2D(256, 2, activation='relu', padding='same')(up7)\n",
        "    merge7 = concatenate([conv3, up7], axis=3)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
        "    up8 = Conv2D(128, 2, activation='relu', padding='same')(up8)\n",
        "    merge8 = concatenate([conv2, up8], axis=3)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
        "    up9 = Conv2D(64, 2, activation='relu', padding='same')(up9)\n",
        "    merge9 = concatenate([conv1, up9], axis=3)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile the U-Net model with additional metrics\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall(), MeanIoU(num_classes=2)])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the U-Net model\n",
        "input_size = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
        "model = unet_model(input_size)\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Define callbacks\n",
        "#earlystopper = EarlyStopping(patience=10, verbose=1)\n",
        "checkpointer = ModelCheckpoint('unet_water_segmentation.keras', verbose=1, save_best_only=True)\n",
        "\n",
        "# Train the model using data generators\n",
        "steps_per_epoch = len(image_train) // BATCH_SIZE\n",
        "validation_steps = len(image_val) // BATCH_SIZE\n",
        "\n",
        "# Train the model using data generators\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=50,\n",
        "    #callbacks=[earlystopper, checkpointer]\n",
        "    callbacks=[checkpointer]\n",
        ")\n",
        "\n",
        "#To download the model, but checkpoint should be saved eitherways\n",
        "#from google.colab import files\n",
        "#files.download('unet_water_segmentation.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results EDA"
      ],
      "metadata": {
        "id": "5b4z04eomFcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZH4vJeRj40f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Get the number of epochs\n",
        "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "\n",
        "    # Specify tick values at intervals of 5, starting from 1\n",
        "    tick_values = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45,50]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(tick_values)  # Set x-ticks to the desired specific values\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Precision\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history.history['precision'], label='Training Precision')\n",
        "    plt.plot(epochs, history.history['val_precision'], label='Validation Precision')\n",
        "    plt.title('Model Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.xticks(tick_values)  # Set x-ticks to the desired specific values\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Recall\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history.history['recall'], label='Training Recall')\n",
        "    plt.plot(epochs, history.history['val_recall'], label='Validation Recall')\n",
        "    plt.title('Model Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xticks(tick_values)  # Set x-ticks to the desired specific values\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot IoU (MeanIoU)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history.history['mean_io_u'], label='Training IoU')\n",
        "    plt.plot(epochs, history.history['val_mean_io_u'], label='Validation IoU')\n",
        "    plt.title('Model IoU')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.xticks(tick_values)  # Set x-ticks to the desired specific values\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the training history\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pts8AWGKVMrm"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.metrics import Precision, Recall, MeanIoU\n",
        "\n",
        "# Load the model from the saved .keras file\n",
        "model = load_model('/content/drive/MyDrive/Advanced DL/unet_water_segmentation.keras', custom_objects={\n",
        "    'Precision': Precision(),\n",
        "    'Recall': Recall(),\n",
        "    'MeanIoU': MeanIoU(num_classes=2)\n",
        "})\n",
        "\n",
        "# Display the model summary to confirm it loaded correctly\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJdkzEqhU_0u"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model on the test set\n",
        "test_steps = len(image_test) // BATCH_SIZE\n",
        "#test_loss, test_acc = model.evaluate(test_gen, steps=test_steps)\n",
        "#print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc, test_precision, test_recall, test_iou = model.evaluate(test_gen, steps=test_steps)\n",
        "\n",
        "# Print test set results\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_acc}')\n",
        "print(f'Test Precision: {test_precision}')\n",
        "print(f'Test Recall: {test_recall}')\n",
        "print(f'Test IoU: {test_iou}')\n",
        "\n",
        "\n",
        "# Clear memory after each epoch to avoid memory overload\n",
        "gc.collect()\n",
        "\n",
        "# Predict and visualize some test samples\n",
        "preds = model.predict(test_gen, steps=test_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCKLOwQsU_oH"
      },
      "outputs": [],
      "source": [
        "# Predict on a single batch of test data\n",
        "test_batch = next(test_gen)\n",
        "test_images, test_masks = test_batch\n",
        "preds = model.predict(test_images)\n",
        "\n",
        "# Plot predictions vs actual masks\n",
        "n = min(5, len(test_images))  # Number of samples to display, up to 5\n",
        "plt.figure(figsize=(12, 3*n))  # Adjusted figure size for a smaller grid\n",
        "\n",
        "for i in range(n):\n",
        "    # Plot the original image\n",
        "    plt.subplot(n, 3, i*3 + 1)\n",
        "    plt.imshow(test_images[i])\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the actual mask\n",
        "    plt.subplot(n, 3, i*3 + 2)\n",
        "    plt.imshow(test_masks[i].squeeze(), cmap='gray')\n",
        "    plt.title('Actual Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the predicted mask\n",
        "    plt.subplot(n, 3, i*3 + 3)\n",
        "    plt.imshow(preds[i].squeeze(), cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout(pad=1.0)  # Adjust padding between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZzprsxPUSMR"
      },
      "source": [
        "## Final evaluation check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "55bTv26NORCr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.metrics import Precision, Recall, MeanIoU\n",
        "\n",
        "# Load the trained U-Net model\n",
        "model = load_model('/content/drive/MyDrive/Advanced DL/Trained Models/unet_water_segmentation.keras', custom_objects={\n",
        "    'Precision': Precision(),\n",
        "    'Recall': Recall(),\n",
        "    'MeanIoU': MeanIoU(num_classes=2)\n",
        "})\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_steps = len(image_test) // BATCH_SIZE\n",
        "test_start_time = time.time()\n",
        "test_loss, test_acc, test_precision, test_recall, test_iou = model.evaluate(test_gen, steps=test_steps)\n",
        "\n",
        "test_time = time.time() - test_start_time\n",
        "\n",
        "# Print test set results\n",
        "print(f'U-Net Evaluation:')\n",
        "print(f'Time: {test_time:.2f} seconds')\n",
        "print(f'Average Test Precision: {test_precision:.4f}')\n",
        "print(f'Average Test Recall: {test_recall:.4f}')\n",
        "print(f'Average Test IoU: {test_iou:.4f}')\n",
        "print(f'Average Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Clear memory after evaluation\n",
        "gc.collect()\n",
        "\n",
        "# Predict and visualize some test samples\n",
        "preds = model.predict(test_gen, steps=test_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4twBElq7SPxj"
      },
      "outputs": [],
      "source": [
        "from model.inference import load_model\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import logging\n",
        "import shutil\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from data_loader.load_data import create_datasets, create_data_loaders\n",
        "from model.inference import load_model\n",
        "from visualizer.visualize import visualize_data\n",
        "\n",
        "def load_config(config_path):\n",
        "    \"\"\"Loads the YAML configuration file.\"\"\"\n",
        "    with open(config_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "def calculate_segmentation_metrics(predictions, targets, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, IoU, and accuracy for binary segmentation with thresholding.\n",
        "    \"\"\"\n",
        "    # Convert model output probabilities to binary class predictions\n",
        "    #predicted_probs = torch.softmax(predictions, dim=1)\n",
        "    #predicted_masks = (predicted_probs[:, 1] > threshold).float()\n",
        "    predicted_masks = []\n",
        "    for prediction in predictions:\n",
        "        mask = torch.argmax(prediction, dim=0)\n",
        "        predicted_masks.append(mask)\n",
        "\n",
        "    predicted_masks = torch.stack(predicted_masks)\n",
        "\n",
        "    # Flatten predictions and targets for pixel-wise comparison\n",
        "    targets = targets.view(-1) #torch.argmax(targets, dim=1) #.view(-1)\n",
        "    predicted_masks = predicted_masks.view(-1)\n",
        "\n",
        "    # Calculate TP, FP, TN, FN\n",
        "    true_positive = (predicted_masks == 1) & (targets == 1)\n",
        "    false_positive = (predicted_masks == 1) & (targets == 0)\n",
        "    false_negative = (predicted_masks == 0) & (targets == 1)\n",
        "    true_negative = (predicted_masks == 0) & (targets == 0)\n",
        "\n",
        "    # Metrics\n",
        "    precision = true_positive.sum().float() / (true_positive.sum() + false_positive.sum() + 1e-6)\n",
        "    recall = true_positive.sum().float() / (true_positive.sum() + false_negative.sum() + 1e-6)\n",
        "    iou = true_positive.sum().float() / (true_positive.sum() + false_positive.sum() + false_negative.sum() + 1e-6)\n",
        "    accuracy = (true_positive.sum().float() + true_negative.sum().float()) / (targets.numel() + 1e-6)\n",
        "\n",
        "    return precision.item(), recall.item(), iou.item(), accuracy.item()\n",
        "\n",
        "\n",
        "def preprocess_image(image_path, resize):\n",
        "    \"\"\"Preprocesses the image for model input.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        return preprocess(img).unsqueeze(0)  # Add batch dimension\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "config_path = 'trainer_config.yaml'\n",
        "config = load_config(config_path)\n",
        "\n",
        "data_loader_config = load_config(config['DATA_LOADER_CONFIG_PATH'])\n",
        "video_path = '/content/drive/MyDrive/Advanced DL/Final Video Results/Input Videos/first.mov'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "output_path = '/content/drive/MyDrive/Advanced DL/Final Video Results/UNET/first_output.mp4'\n",
        "\n",
        "#Set threshold\n",
        "change_threshold = 0.7\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video details\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Process each frame\n",
        "frame_count = 0\n",
        "previous_water_area = None\n",
        "previous_frame = None\n",
        "previous_mask = None\n",
        "\n",
        "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = load_model(config['MODEL_CONFIG_PATH'], device)\n",
        "model = model.to(device)\n",
        "pred_mask_list = []\n",
        "max_idx = -1\n",
        "max_val = 0\n",
        "image_list = []\n",
        "#pre_image_list = []\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_list.append(frame)\n",
        "    cv2.imwrite('frame.jpg', frame)\n",
        "    img_tensor = preprocess_image('frame.jpg', [256, 256])\n",
        "    img_tensor = img_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(img_tensor)\n",
        "\n",
        "    if predictions.shape[1] == 2:  # Assuming binary segmentation with 2 channels\n",
        "        predictions = torch.softmax(predictions, dim=1)\n",
        "\n",
        "\n",
        "    threshold = 0.7\n",
        "    predicted_mask = (predictions[:, 1, :, :] > threshold).long()\n",
        "\n",
        "    predicted_mask = predicted_mask.squeeze().cpu().numpy() * 255\n",
        "    predicted_mask = predicted_mask.astype(np.uint8)\n",
        "    pred_mask_list.append(predicted_mask)\n",
        "    current_water_area = np.sum(predicted_mask)\n",
        "\n",
        "    if current_water_area > max_val:\n",
        "        max_idx = frame_count\n",
        "        max_val = current_water_area\n",
        "\n",
        "    #if frame_count == 100:\n",
        "    #    break\n",
        "    print(frame_count)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "for i in range(len(pred_mask_list)):\n",
        "\n",
        "    intersection_area = np.sum(np.logical_and(pred_mask_list[i], pred_mask_list[max_idx]))*255\n",
        "    intersection_ratio = intersection_area / max_val\n",
        "    current_mask = pred_mask_list[i]\n",
        "    area_pixels = np.sum(current_mask)  # Count active pixels in the mask\n",
        "    current_level = area_pixels / current_mask.size  # Normalize by total pixels (to get a fraction)\n",
        "    current_area = area_pixels  # Area in pixels, can convert to real-world units if needed\n",
        "\n",
        "    # Create a graphical representation of water level\n",
        "    frame = image_list[i]\n",
        "    height, width, _ = frame.shape\n",
        "    water_line_y = int(height * (1 - (current_level)))  # Normalize height for visualization\n",
        "\n",
        "    cv2.line(frame, (0, water_line_y), (width, water_line_y), (255, 0, 0), 5)  # Red line\n",
        "\n",
        "    area_bar_height = int(height * (current_area / (height * width)))  # Normalize height for bar\n",
        "    cv2.rectangle(frame, (width - 100, height), (width - 50, height - area_bar_height), (0, 255, 0), -1)  # Green bar\n",
        "\n",
        "    if intersection_ratio < threshold:\n",
        "            alert_text = \"ALERT: Water Level Low!\"\n",
        "            cv2.putText(frame, alert_text, (30, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)  # Red text\n",
        "            red_overlay = frame.copy()\n",
        "            red_overlay[:, :] = (0, 0, 255)  # Red color in BGR format\n",
        "\n",
        "            # Blend the red overlay with the current frame\n",
        "            alpha = 0.4  # Transparency level for the overlay\n",
        "            frame = cv2.addWeighted(red_overlay, alpha, frame, 1 - alpha, 0)  # Blend red overlay with the frame\n",
        "\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Video segmentation completed and saved to\", output_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywWmJImUU4JH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.metrics import Precision, Recall, MeanIoU\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the trained U-Net model\n",
        "model = load_model('/content/drive/MyDrive/Advanced DL/Trained Models/unet_water_segmentation.keras', custom_objects={\n",
        "    'Precision': Precision(),\n",
        "    'Recall': Recall(),\n",
        "    'MeanIoU': MeanIoU(num_classes=2)\n",
        "})\n",
        "\n",
        "# Load the video\n",
        "video_path = '/content/drive/MyDrive/Advanced DL/Final Video Results/Input Videos/fourth.mov'\n",
        "output_path = '/content/drive/MyDrive/Advanced DL/Final Video Results/UNET/fourth_output.mp4'\n",
        "\n",
        "# Set threshold\n",
        "change_threshold = 0.7\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video details\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Process each frame\n",
        "frame_count = 0\n",
        "previous_water_area = None\n",
        "previous_mask = None\n",
        "max_water_area = 0\n",
        "max_mask = None\n",
        "frame_list = []\n",
        "pred_mask_list = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_list.append(frame)\n",
        "\n",
        "    # Resize frame to match model input size (assuming model expects 256x256)\n",
        "    resized_frame = cv2.resize(frame, (256, 256))\n",
        "\n",
        "    # Preprocess the frame for model prediction\n",
        "    input_frame = resized_frame / 255.0  # Normalize the image\n",
        "    input_frame = np.expand_dims(input_frame, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Predict the mask\n",
        "    predicted_mask = model.predict(input_frame)[0]\n",
        "\n",
        "    # Post-process the predicted mask\n",
        "    predicted_mask = (predicted_mask.squeeze() > 0.5).astype(np.uint8)  # Threshold the mask\n",
        "    predicted_mask = cv2.resize(predicted_mask, (width, height))  # Resize mask to original frame size\n",
        "\n",
        "    # Calculate the water area in the current frame\n",
        "    current_water_area = np.sum(predicted_mask)\n",
        "    pred_mask_list.append(predicted_mask)\n",
        "\n",
        "    # Track the frame with the maximum water area\n",
        "    if current_water_area > max_water_area:\n",
        "        max_water_area = current_water_area\n",
        "        max_mask = predicted_mask\n",
        "\n",
        "    frame_count += 1\n",
        "    if frame_count % 10 == 0:\n",
        "        print(f'Processed {frame_count} frames...')\n",
        "\n",
        "# Apply visualization to each frame\n",
        "for i in range(len(frame_list)):\n",
        "    frame = frame_list[i]\n",
        "    predicted_mask = pred_mask_list[i]\n",
        "\n",
        "    # Calculate intersection ratio\n",
        "    intersection_area = np.sum(np.logical_and(predicted_mask, max_mask))\n",
        "    intersection_ratio = intersection_area / (max_water_area + 1e-6)\n",
        "\n",
        "    # Create a graphical representation of water level\n",
        "    area_pixels = np.sum(predicted_mask)  # Count active pixels in the mask\n",
        "    current_level = area_pixels / predicted_mask.size  # Normalize by total pixels (to get a fraction)\n",
        "    current_area = area_pixels  # Area in pixels, can convert to real-world units if needed\n",
        "\n",
        "    # Draw water level indicator\n",
        "    height, width, _ = frame.shape\n",
        "    water_line_y = int(height * (1 - current_level))  # Normalize height for visualization\n",
        "    cv2.line(frame, (0, water_line_y), (width, water_line_y), (255, 0, 0), 5)  # Red line\n",
        "\n",
        "    # Draw area bar indicator\n",
        "    area_bar_height = int(height * (current_area / (height * width)))  # Normalize height for bar\n",
        "    cv2.rectangle(frame, (width - 100, height), (width - 50, height - area_bar_height), (0, 255, 0), -1)  # Green bar\n",
        "\n",
        "    # Add ALERT text and overlay if intersection ratio is below the threshold\n",
        "    if intersection_ratio < change_threshold:\n",
        "        alert_text = \"ALERT: Water Level Low!\"\n",
        "        cv2.putText(frame, alert_text, (30, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)  # Red text\n",
        "        red_overlay = frame.copy()\n",
        "        red_overlay[:, :] = (0, 0, 255)  # Red color in BGR format\n",
        "        alpha = 0.4  # Transparency level for the overlay\n",
        "        frame = cv2.addWeighted(red_overlay, alpha, frame, 1 - alpha, 0)  # Blend red overlay with the frame\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Video segmentation completed and saved to\", output_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}